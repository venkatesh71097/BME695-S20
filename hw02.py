# -*- coding: utf-8 -*-
"""hw02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rJjw5vstoBpllggEfn-s3oZF0OmmLxj_
"""

import torch
import torchvision
import torchvision.transforms as transforms
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import utils
from sklearn.preprocessing import OneHotEncoder 
from torch.utils.data.sampler import SubsetRandomSampler
onehotencoder = OneHotEncoder()
f = open('output.txt', "w+")
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) 
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,shuffle=True, num_workers=2)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=True, num_workers=2)

l = [] 
l = np.array(l, dtype='uint8')

m = []
m = np.array(m, dtype='uint8')

class cifardataset(Dataset):
    def __init__(self, dataset, transform = None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)
                
c = 0
d = 0

cifar = cifardataset(dataset = trainset, transform = transform)
for i in range(len(cifar.dataset)):
    if cifar.dataset[i][1] == 3 or cifar.dataset[i][1] == 5:
        l = np.append(l, i)
        c = c + 1

trset = torch.utils.data.Subset(cifar.dataset, l)
subset_loader = torch.utils.data.DataLoader(trset, batch_size=10, shuffle=True)

for i in range(len(testset)):
    if testset[i][1] == 3 or testset[i][1] == 5:
        m = np.append(m, i)
        d = d + 1

tstset = torch.utils.data.Subset(testset, m)
tst_loader = torch.utils.data.DataLoader(tstset, batch_size=10, shuffle=True)
        
dtype = torch.float
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
N, D_in, H1, H2, D_out = 8, 3*32*32, 1000, 256, 2
# Randomly initialize weights
w1 = torch.randn(D_in, H1, device=device, dtype=dtype)
w2 = torch.randn(H1, H2, device=device, dtype=dtype)
w3 = torch.randn(H2, D_out, device=device, dtype=dtype)
learning_rate = 1e-8
i =0

iteration = []
iteration = np.array(iteration, dtype='uint8')
loss_value = [] 
l = np.array(l, dtype='float64')
for t in range(100):
    loss_acc = 0
    for i, data in enumerate(subset_loader):
        inputs, lbl = data
        #inputs = ip.numpy()
        lab = lbl.numpy()

        for i in range(len(lab)):
            if lab[i] == 3: 
                lab[i] = 0
            if lab[i] == 5:
                lab[i] = 1
        nb_classes = 2
        targets = np.array(lab.reshape(-1), dtype='uint8')
        labels = np.eye(nb_classes, dtype ='uint8')[targets]
        labels = torch.from_numpy(labels)
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        x = inputs.view(inputs.size(0), -1)
        h1 = x.mm(w1) #In numpy, you would say h1 = x.dot(w1)
        h1_relu = h1.clamp(min=0)
        h2 = h1_relu.mm(w2)
        h2_relu = h2.clamp(min=0)
        y_pred = h2_relu.mm(w3)
        # Compute and print loss
        loss = (y_pred - labels).pow(2).sum().item()
        loss_acc = loss_acc + loss
        y_error = 2 * (y_pred - labels)
        grad_w3 = h2_relu.t().mm(y_error) #<<<<<< Gradient of Loss w.r.t w3
        h2_error = y_error.mm(w3.t()) # backpropagated error to the h2 hidden layer
        h2_error[h2 < 0] = 0 # We set those elements of the backpropagated error
        grad_w2 = h1_relu.t().mm(h2_error) #<<<<<< Gradient of Loss w.r.t w2
        h1_error = h2_error.mm(w2.t()) # backpropagated error to the h1 hidden layer
        h1_error[h1 < 0] = 0 # We set those elements of the backpropagated error
        grad_w1 = x.t().mm(h1_error) #<<<<<< Gradient of Loss w.r.t w2
        # Update weights using gradient descent
        w1 -= learning_rate * grad_w1
        w2 -= learning_rate * grad_w2
        w3 -= learning_rate * grad_w3
    print("Epoch " + str(t) + ": " + str(loss_acc / 10000))
    f.write("Epoch " + str(t) + ": " + str(loss_acc / 10000) + "\n")
    loss_value = np.append(loss_value, loss_acc/10000)
    iteration = np.append(iteration, t)

print(" ")
f.write("\n")

for t in range(1):
    tst_loss_acc = 0
    k = 0
    for i, data in enumerate(tst_loader):
        inputs, lbl = data
        #inputs = ip.numpy()
        lab = lbl.numpy()

        for i in range(len(lab)):
            if lab[i] == 3: 
                lab[i] = 0
            if lab[i] == 5:
                lab[i] = 1
        nb_classes = 2
        targets = np.array(lab.reshape(-1), dtype='uint8')
        labels = np.eye(nb_classes, dtype ='uint8')[targets]
        labels = torch.from_numpy(labels)
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        x = inputs.view(inputs.size(0), -1)
        h1 = x.mm(w1) #In numpy, you would say h1 = x.dot(w1)
        h1_relu = h1.clamp(min=0)
        h2 = h1_relu.mm(w2)
        h2_relu = h2.clamp(min=0)
        y_pred = h2_relu.mm(w3)
        for i in range(len(y_pred)):
            if y_pred[i][0] > y_pred[i][1]:
                y_pred[i][0] = 1
                y_pred[i][1] = 0
            elif y_pred[i][0] < y_pred[i][1]:
                y_pred[i][0] = 0
                y_pred[i][1] = 1
                
        for i in range(len(y_pred)):
            if y_pred[i][0] == labels[i][0] and y_pred[i][1] == labels[i][1]: 
                k = k+1

        # Compute and print loss
        loss = (y_pred - labels).pow(2).sum().item()
        tst_loss_acc = tst_loss_acc + loss
        y_error = y_pred - labels
    accuracy = (k / 2000) * 100
    print("Test Accuracy: " + str(accuracy))
    f.write("Test Accuracy: " + str(accuracy) + "% \n")
    
plt.plot(iteration, loss_value)
f.close()